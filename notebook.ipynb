{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import fabs\n",
    "from numpy.core.numeric import outer\n",
    "from util.data_loader import BCI2aDataset, PhysioDataset\n",
    "from util.transforms import filterBank, gammaFilter, MSD, Energy_Wavelet, Normal\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from model.svm import SVM\n",
    "from model.hmm import HMM\n",
    "from model.energyNN import NeuralNetwork\n",
    "from model.csp_lda import CSP_LDA\n",
    "from model.mixed_fbcnet import MIXED_FBCNet\n",
    "from model.cp_mixednet import CP_MixedNet\n",
    "from model.mi_cnn import MI_CNN\n",
    "from model.cnn_lstm import CNNLSTM\n",
    "#from model.min2net import MIN2NET\n",
    "from model.eegnet import EEGNet\n",
    "from model.fbcnet import FBCNet\n",
    "from torch import nn\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import accuracy_score, det_curve\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "CONFIG_PATH = \"../config/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load yaml configuration file\n",
    "def load_config(config_name):\n",
    "    with open(os.path.join(CONFIG_PATH, config_name)) as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device, dtype=torch.float), y.to(device)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    correct /= size\n",
    "    print(f\"Train Error: \\n Accuracy: {(100*correct):>0.1f}%\\n\")\n",
    "\n",
    "def test(dataloader, model, loss_fn, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device, dtype=torch.float), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    epoch_acc = 100.0*correct\n",
    "    return epoch_acc\n",
    "\n",
    "def valid(dataloader, model):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    all_y= []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device, dtype=torch.float), y.to(device)\n",
    "            pred = model(X)\n",
    "            scores.extend(10**pred.cpu().numpy()[:,1:])\n",
    "            all_y.extend(y.cpu().numpy())\n",
    "    for i, score in enumerate(scores):\n",
    "        scores[i] = score[0]\n",
    "    fpr, fnr, thresholds = det_curve(all_y, scores, pos_label=1)\n",
    "    EER = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    print(\"EER: {}%\".format(EER*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    config = load_config(\"mixed_fbcnet_config.yaml\")\n",
    "\n",
    "    if config[\"transform\"][\"name\"] == \"nBand\":    \n",
    "        filterTransform = filterBank([[4,8],[8,12],[12,16],[16,20],[20,24],[24,28],[28,32],[32,36],[36,40]], 160)\n",
    "    \n",
    "    \n",
    "    if config[\"dataset\"][\"name\"] == \"PhysioDataset\":\n",
    "        train_data = PhysioDataset(subject=config[\"train\"][\"subject\"], path=config[\"dataset\"][\"location\"], train=\"train\", transform=filterTransform, select_channel=config[\"channel\"][\"select\"], use_channel_no=config[\"channel\"][\"number\"], preprocess=config[\"dataset\"][\"preprocess\"])\n",
    "        if config[\"train\"][\"batch_size\"] == \"all\":\n",
    "            batch_size = len(train_data)\n",
    "        else:\n",
    "            batch_size = config[\"train\"][\"batch_size\"]\n",
    "        train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True) \n",
    "        \n",
    "        channels = train_data.channels()\n",
    "\n",
    "        intra_test_data = PhysioDataset(subject=config[\"train\"][\"subject\"], path=config[\"dataset\"][\"location\"], train=\"intra_test\", transform=filterTransform, channels=channels, preprocess=config[\"dataset\"][\"preprocess\"])\n",
    "        if config[\"evaluate\"][\"batch_size\"] == \"all\":\n",
    "            batch_size = len(intra_test_data)\n",
    "        else:\n",
    "            batch_size = config[\"evaluate\"][\"batch_size\"]\n",
    "        intra_test_dataloader = DataLoader(intra_test_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "        inter_test_data = PhysioDataset(subject=config[\"train\"][\"subject\"], path=config[\"dataset\"][\"location\"], train=\"inter_test\", transform=filterTransform, channels=channels, preprocess=config[\"dataset\"][\"preprocess\"])\n",
    "        if config[\"evaluate\"][\"batch_size\"] == \"all\":\n",
    "            batch_size = len(inter_test_data)\n",
    "        else:\n",
    "            batch_size = config[\"evaluate\"][\"batch_size\"]\n",
    "        inter_test_dataloader = DataLoader(inter_test_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    \n",
    "    # save channels\n",
    "    with open(str(config[\"train\"][\"subject\"])+'.txt', 'w') as filehandle:\n",
    "        for channel in channels:\n",
    "            filehandle.write('%s\\n' % channel)\n",
    "\n",
    "    if config[\"model\"][\"name\"] in [\"FBCNet\", \"MICNN\", \"CNN_LSTM\", \"CP_MIXEDNET\", \"EEGNet\", \"MIXED_FBCNet\"]:\n",
    "        if config[\"model\"][\"name\"] == \"MIXED_FBCNet\":\n",
    "            model = MIXED_FBCNet(nChan=config[\"channel\"][\"number\"]).to(device)\n",
    "        \n",
    "\n",
    "        loss_fn = nn.NLLLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config[\"optimizer\"][\"initial_lr\"], weight_decay=config[\"optimizer\"][\"weight_decay\"])\n",
    "        exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.2)\n",
    "        best_intra_acc = 0\n",
    "        best_inter_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "util.data_loader.PhysioDataset"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(intra_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MIXED_FBCNet(\n",
       "  (channelProj): Sequential(\n",
       "    (0): Conv2dWithConstraint(10, 30, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): swish()\n",
       "  )\n",
       "  (shapeTrans): Sequential(\n",
       "    (0): Conv2dWithConstraint(30, 30, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): swish()\n",
       "  )\n",
       "  (spatialConv): Sequential(\n",
       "    (0): Conv2dWithConstraint(9, 288, kernel_size=(10, 1), stride=(1, 1), groups=9)\n",
       "    (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): swish()\n",
       "  )\n",
       "  (pointWise): Sequential(\n",
       "    (0): Conv2dWithConstraint(288, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): swish()\n",
       "  )\n",
       "  (varLayer): LogVarLayer()\n",
       "  (standTemporalLayer): Sequential(\n",
       "    (0): Dropout2d(p=0.5, inplace=False)\n",
       "    (1): Conv2dWithConstraint(288, 28, kernel_size=(1, 32), stride=(32, 32), bias=False)\n",
       "    (2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): swish()\n",
       "    (4): MaxPool2d(kernel_size=(1, 4), stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv2dWithConstraint(316, 316, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (2): BatchNorm2d(316, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (lastLayer): Sequential(\n",
       "    (0): LinearWithConstraint(in_features=1580, out_features=2, bias=True)\n",
       "    (1): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MIXED_FBCNet(nChan=10).to(device)\n",
    "model.load_state_dict(torch.load(\"../trained/4_96.8_96.0_20.pth\")) # Load the weights\n",
    "model.to(device) # Send the model to device\n",
    "model.eval() # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'intra_test_data' is your data loader\n",
    "data_iter = iter(intra_test_data)\n",
    "single_sample = next(data_iter)\n",
    "\n",
    "# Unpack the data and labels\n",
    "data, target = single_sample\n",
    "\n",
    "# Move the data and label to GPU if available\n",
    "data = data.to(device)\n",
    "target = target.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset info \n",
    "\n",
    "train: run = [\"04\", \"06\", \"08\", \"10\"]\n",
    "            \n",
    "intra / inter  : run = [\"12\", \"14\"]\n",
    "\n",
    "BECAUSE , dataset description (thats why he mentions 6 runs !! apparently each .edf is 15 trials ??? each trial has an imagination segment):\n",
    "\n",
    "In summary, the experimental runs were:\n",
    "\n",
    "1   Baseline, eyes open\n",
    "\n",
    "2   Baseline, eyes closed\n",
    "\n",
    "3   Task 1 (open and close left or right fist)\n",
    "\n",
    "4   Task 2 (imagine opening and closing left or right fist) -------------\n",
    "\n",
    "5   Task 3 (open and close both fists or both feet)\n",
    "\n",
    "6   Task 4 (imagine opening and closing both fists or both feet) ------------\n",
    "\n",
    "7   Task 1\n",
    "\n",
    "8   Task 2 --------------\n",
    "\n",
    "9   Task 3\n",
    "\n",
    "10  Task 4 ---------------\n",
    "\n",
    "11  Task 1\n",
    "\n",
    "12  Task 2 ---------------\n",
    " \n",
    "13  Task 3\n",
    "\n",
    "14  Task 4 ----------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 640, 9])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dim : torch.Size([10, 640, 9])\n",
    "\n",
    "From article : 10 channels , 640 time points, 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is 1 ? 1 is true 0 is false (bona fide vs attack) they use Run 12 and 14 for non train lab=1.\n",
    "\n",
    "then they add attackers which are not subject 4: for intra we are adding subject 1-21 except 4 as attackers.\n",
    "\n",
    "hence 20 attackers and 2 bona fide ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output: tensor([[-3.8718, -0.0210]])\n",
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "single_sample, label = next(iter(intra_test_data))\n",
    "single_sample = single_sample.to(device)\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    output = model(single_sample.unsqueeze(0))\n",
    "\n",
    "print(f\"Raw output: {output}\")\n",
    "\n",
    "# Convert raw output to predicted class index\n",
    "predicted_class = output.argmax(dim=1)\n",
    "\n",
    "print(f\"Predicted class: {predicted_class.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.723920  [    0/  155]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jacobo\\anaconda3\\envs\\eegBIN\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.273518  [   40/  155]\n",
      "loss: 0.303911  [   80/  155]\n",
      "loss: 1.015434  [  120/  155]\n",
      "Train Error: \n",
      " Accuracy: 67.1%\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.153949  [    0/  155]\n",
      "loss: 0.953507  [   40/  155]\n",
      "loss: 0.423350  [   80/  155]\n",
      "loss: 0.556763  [  120/  155]\n",
      "Train Error: \n",
      " Accuracy: 73.5%\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.268919  [    0/  155]\n",
      "loss: 0.476662  [   40/  155]\n",
      "loss: 0.185683  [   80/  155]\n",
      "loss: 0.303036  [  120/  155]\n",
      "Train Error: \n",
      " Accuracy: 81.3%\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.634289  [    0/  155]\n",
      "loss: 0.136143  [   40/  155]\n",
      "loss: 0.480330  [   80/  155]\n",
      "loss: 0.661132  [  120/  155]\n",
      "Train Error: \n",
      " Accuracy: 80.6%\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.717558  [    0/  155]\n",
      "loss: 0.168494  [   40/  155]\n",
      "loss: 1.286950  [   80/  155]\n",
      "loss: 0.230716  [  120/  155]\n",
      "Train Error: \n",
      " Accuracy: 85.2%\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.862148  [    0/  155]\n",
      "loss: 0.296063  [   40/  155]\n",
      "loss: 0.049537  [   80/  155]\n",
      "loss: 0.096883  [  120/  155]\n",
      "Train Error: \n",
      " Accuracy: 85.8%\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.555516  [    0/  155]\n",
      "loss: 0.424203  [   40/  155]\n",
      "loss: 0.705997  [   80/  155]\n",
      "loss: 0.005857  [  120/  155]\n",
      "Train Error: \n",
      " Accuracy: 83.9%\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.051016  [    0/  155]\n",
      "loss: 0.490040  [   40/  155]\n",
      "loss: 0.202123  [   80/  155]\n",
      "loss: 1.133913  [  120/  155]\n",
      "Train Error: \n",
      " Accuracy: 85.8%\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.096181  [    0/  155]\n",
      "loss: 0.022136  [   40/  155]\n",
      "loss: 0.187715  [   80/  155]\n",
      "loss: 0.026213  [  120/  155]\n",
      "Train Error: \n",
      " Accuracy: 87.1%\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.131521  [    0/  155]\n",
      "loss: 0.509257  [   40/  155]\n",
      "loss: 1.152406  [   80/  155]\n",
      "loss: 0.060358  [  120/  155]\n",
      "Train Error: \n",
      " Accuracy: 84.5%\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "        \n",
    "        for t in range(config[\"train\"][\"epochs\"]):\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "            train(train_dataloader, model, loss_fn, optimizer)\n",
    "            exp_lr_scheduler.step()\n",
    "            if (t > 50):\n",
    "                intra_acc = test(intra_test_dataloader, model, loss_fn, t)\n",
    "                inter_acc = test(inter_test_dataloader, model, loss_fn, t)\n",
    "                if t > 250:\n",
    "                    best_inter_acc = inter_acc\n",
    "                    valid(intra_test_dataloader, model)\n",
    "                    valid(inter_test_dataloader, model)\n",
    "                    #torch.save(model.state_dict(), \"../trained/\"+str(config[\"train\"][\"subject\"])+\"_\"+str(intra_acc)+\"_\"+str(inter_acc)+\"_20\"+\".pth\")\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
